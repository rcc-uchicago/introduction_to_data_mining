# -*- coding: utf-8 -*-
"""NYT_news_mining_train_by_sentiments.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UhA9r4w3jaDmuWsG3Wh1BUFmseZ6NMuz

# NYT daily articles have been prepared in a seperate notebooks
"""

import tensorflow as tf
print(tf.__version__)

import pandas as pd
import datetime
import random
from datetime import date, timedelta
import plotly.express as px
import pickle
import nltk
nltk.download('vader_lexicon')
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import numpy as np
from nltk import word_tokenize
nltk.download('stopwords')
nltk.download('punkt')
from nltk.corpus import stopwords
import string

"""### The NYT headline data was processed in a seperate notebook. Here, I load the processed as a pickle binary file."""

# load file1.pkl binary file
with open('/content/drive/MyDrive/Colab_Notebooks/does_news_reflect_reality_covid19_financial_markets/daily_articles.pkl', 'rb') as f:
    daily_articles = pickle.load(f) # dictionary based on date, e.g., daily_articles['2020-01-07']

"""#### Construct 2 lists, headline_labels = [] and headlines = []"""

# A self-supervising approach is taken. Using vader to obtain the headline_labels
sid = SentimentIntensityAnalyzer()
a = 'This was a good movie.'
x = sid.polarity_scores(daily_articles['2020-11-03'][10])
sid.polarity_scores('this is not great')

headline_labels = []
headlines = []

for date in daily_articles:

  number_articles = len(daily_articles[date])
  for article_number in range(0, number_articles):
    article = daily_articles[date][article_number]
    compound_article_sentiment = sid.polarity_scores(article)['compound']
    if compound_article_sentiment > 0:
      headline_labels.append(1)
      headlines.append(article)
    elif compound_article_sentiment < 0:
      headline_labels.append(0)
      headlines.append(article)

"""### Pre-processing the data, lemmatization, stemming, and stop word removal

#### Stopwords and punctuations removal
"""

stopwords_hand_generated = [ "a", "about", "above", "after", "again", "against",\
             "all", "am", "an", "and", "any", "are", "as", "at", "be",\
             "because", "been", "before", "being", "below", "between",\
             "both", "but", "by", "could", "did", "do", "does", "doing",\
             "down", "during", "each", "few", "for", "from", "further",\
             "had", "has", "have", "having", "he", "he'd", "he'll",\
             "he's", "her", "here", "here's", "hers", "herself", "him", "himself",\
             "his", "how", "how's", "i", "i'd", "i'll", "i'm", "i've", "if", "in",\
             "into", "is", "it", "it's", "its", "itself", "let's", "me", "more",\
             "most", "my", "myself", "nor", "of", "on", "once", "only", "or", "other",\
             "ought", "our", "ours", "ourselves", "out", "over", "own", "same",\
             "she", "she'd", "she'll", "she's", "should", "so", "some", "such",\
             "than", "that", "that's", "the", "The", "their", "theirs", "them", "themselves",\
             "then", "there", "there's", "these", "they", "they'd", "they'll",\
             "they're", "they've", "this", "those", "through", "to", "too", "under", "until",\
             "up", "very", "was", "we", "we'd", "we'll", "we're", "we've", "were", "what",\
             "what's", "when", "when's", "where", "where's", "which", "while", "who", "who's",\
             "whom", "why", "why's", "with", "would", "you", "you'd", "you'll",\
             "you're", "you've", "your", "yours", "yourself", "yourselves" ]


headline_labels_sw_removal = []
def stopwords_removal_naive(raw_headlines):
  headlines_sw_removal = []
  for sentence in headlines:
    print('before: ' + sentence)
    for word in stopwords:
      token = " " + word + " "
      sentence = sentence.replace(token, " ")
      sentence = sentence.replace("  ", " ")
      headlines_sw_removal.append(sentence)
    print('after:  ' + sentence)
  return headlines_sw_removal

sent = "this 'is a foo': bar, bar black sheep."
stops = set(stopwords.words('english') + list(string.punctuation)+["What's", "We're", "What's", "'s", "‘", "’"])

#[i for i in word_tokenize(sent.lower()) if i not in stop]
#['foo', 'bar', 'bar', 'black', 'sheep']

def stopwords_removal(raw_headlines):
  headlines_sw_removal = []
  for sentence in raw_headlines:
    #print('before: ' + sentence)
    
    sentence_lower = word_tokenize(sentence.lower())
    #print(sentence_lower)
    sentence_new=[]
    for word in sentence_lower:
      word.replace("'s", "")
      if word not in stops:
        sentence_new.append(word)
    sentence_final = ' '.join(sentence_new) 
    # strip single quotation mark, with replace()
    sentence_final = sentence_final.replace("'", "")
    sentence_final = sentence_final.replace("`", "") 
    sentence_final = sentence_final.replace("‘", "") 
    sentence_final = sentence_final.replace("’", "") 
    
    headlines_sw_removal.append(sentence_final)
    #print('after:  '+sentence_final)
  return headlines_sw_removal

headlines_sw_removal = stopwords_removal(headlines)

"""#### Stemming"""

# import these modules
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
nltk.download('wordnet')
from nltk.stem.lancaster import LancasterStemmer
lans = LancasterStemmer()   # did not remove plurals and verb tenses in the test
from nltk.stem import WordNetLemmatizer

ps = PorterStemmer()
lemmatizer = WordNetLemmatizer()  # did not remove plurals and verb tenses in the test

ps.stem("A is not a good idea")
lemmatizer.lemmatize("worked as")
lans.stem("many thanks")
x=ps.stem("Trump's attitude")
x.replace("'s", "")

def stem_headlines(raw_headlines):
  headlines_stemmed = []
  for sentence in raw_headlines:
    #print('Before   :'+sentence)
    sentence_new = ps.stem(sentence)
    #if sentence != lemmatizer.lemmatize(sentence):
     # print('Before   :'+sentence)
      #print('lemmatize:'+lemmatizer.lemmatize(sentence))  
    # lemmatizaer is not removing plurals, or verb tenses
    #print('lemmatize:'+lemmatizer.lemmatize(sentence))
    # Lancaster is not removing plurals, or verb tenses
    #print('Lancaster:'+lans.stem(sentence))
    #print('stem     :'+sentence_new)
    headlines_stemmed.append(sentence_new)
  return headlines_stemmed

headlines_stemmed = stem_headlines(headlines_sw_removal)

"""# How often covid or coronavarius is mentioned in the news"""

index_list=[]
for index, headline in enumerate(headlines_stemmed):
  if ('covid' in headline) or ('coronavirus' in headline):
    index_list.append(index)
index_list
covid_list = [headline_labels[index] for index in index_list]
"{:.2%} of NYT covid news contain negative sentiment".format(1-sum(covid_list)/len(covid_list))

index_list=[]
for index, headline in enumerate(headlines_stemmed):
  if 'trump' in headline:
    index_list.append(index)
index_list
covid_list = [headline_labels[index] for index in index_list]
"{:.2%} of NYT Trump related news contain negative sentiment".format(1-sum(covid_list)/len(covid_list))

index_list=[]
for index, headline in enumerate(headlines_stemmed):
  if 'biden' in headline:
    index_list.append(index)
index_list
covid_list = [headline_labels[index] for index in index_list]
"{:.2%} of NYT Biden related news contain negative sentiment".format(1-sum(covid_list)/len(covid_list))

index_list=[]
for index, headline in enumerate(headlines_stemmed):
  if ('republican' in headline) or ('tea party' in headline):
    index_list.append(index)
index_list
covid_list = [headline_labels[index] for index in index_list]
"{:.2%} of NYT republican related news contain negative sentiment".format(1-sum(covid_list)/len(covid_list))

index_list=[]
for index, headline in enumerate(headlines_stemmed):
  if ('democrat' in headline):
    index_list.append(index)
index_list
covid_list = [headline_labels[index] for index in index_list]
"{:.2%} of NYT democrats related news contain negative sentiment".format(1-sum(covid_list)/len(covid_list))

"Normalized trend of NYT lean demotrats is {:}%".format((58.42 + (100-45))/2)

"Normalized trend of NYT lean demotrats is {:}%".format((45+ (100-58.42))/2)

"""# Construct the word embedding learning"""

# source: data source
# 
def split_data(source_headlines, source_labels, split_ratio):
  """
  source: data source
  split_ratio: 0< split_ratio < 1
  return train_headlines, train_labels, test_headlines, test_labels
  """
  train_headlines=[]
  train_labels=[]
  test_headlines=[]
  test_labels=[]
  counter = round(len(source_headlines)*split_ratio)
  source_no = len(source_headlines)

  # For fairness, randomly sample
  for index in random.sample(range(0, source_no), source_no):
    if index < counter:
      train_headlines.append(source_headlines[index])
      train_labels.append(source_labels[index])
    else:
      test_headlines.append(source_headlines[index])
      test_labels.append(source_labels[index])
  return train_headlines, train_labels, test_headlines, test_labels

train_headlines, train_labels, test_headlines, test_labels = split_data(headlines_stemmed, headline_labels, 0.8)

vocab_size = 10000
embedding_dim = 16
max_length = 26
trunc_type='post'
oov_tok = "<OOV>"


from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)
#tokenizer = Tokenizer(oov_token=oov_tok)
tokenizer.fit_on_texts(train_headlines)
word_index = tokenizer.word_index
sequences = tokenizer.texts_to_sequences(train_headlines)
padded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type)
#padded = pad_sequences(sequences, truncating=trunc_type)


testing_sequences = tokenizer.texts_to_sequences(test_headlines)
testing_padded = pad_sequences(testing_sequences,maxlen=max_length)

reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])

def decode_review(text):
    return ' '.join([reverse_word_index.get(i, '?') for i in text])

print(decode_review(padded[3]))
print(train_headlines[3])

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),
    #tf.keras.layers.Flatten(),
     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),
    tf.keras.layers.Dense(6, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
model.summary()

train_labels_final = np.array(train_labels)
test_labels_final = np.array(test_labels)

"""#### Both (padded, train_labels_final) need to be numpy array"""

num_epochs = 5
model.fit(padded, train_labels_final, epochs=num_epochs, validation_data=(testing_padded, test_labels_final))

e = model.layers[0]
weights = e.get_weights()[0]
print(weights.shape) # shape: (vocab_size, embedding_dim)

import io

out_v = io.open('vecs.tsv', 'w', encoding='utf-8')
out_m = io.open('meta.tsv', 'w', encoding='utf-8')
for word_num in range(1, vocab_size):
  word = reverse_word_index[word_num]
  embeddings = weights[word_num]
  out_m.write(word + "\n")
  out_v.write('\t'.join([str(x) for x in embeddings]) + "\n")
out_v.close()
out_m.close()

try:
  from google.colab import files
except ImportError:
  pass
else:
  files.download('vecs.tsv')
  files.download('meta.tsv')

daily_articles['2020-09-01']

articles_df =  pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/daily_articles.csv')

articles_df

articles_df['article count'].sum()/578

"""# Interactive plots"""

# Function to plot interactive plot
def interactive_plot(dfs, title, names):
  fig = px.line(title = title)
  for i in range(0, len(dfs)):
    df = dfs[i]
    fig.add_scatter(x = df['date'], y = df['article count'], name = names[i])
  fig.show()

interactive_plot([articles_df], 'NYT', 'NYT')

"""# Date testing module"""

# Produce a data generator
from datetime import date, timedelta
def daterange(start_date, end_date):
    for n in range(int((end_date - start_date).days)):
        yield start_date + timedelta(n)

# testing :
for year in [2020, 2021]:
  for month in range(1, 13):

    if (year == 2021) & (month==12):
      break
    start_date = date(year, month, 1)
    if (month==12) & (year == 2020):
      end_date = date(year+1, 1, 1)
    else:
      end_date = date(year, month+1, 1)
    for single_date in daterange(start_date, end_date):
        x = single_date.strftime("%Y-%m-%d")
        print(x)

year

month

float(date.today().strftime("%m"))

